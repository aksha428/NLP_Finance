{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbeROSdhx_nI"
      },
      "source": [
        "!pip install transformers\n",
        "import os\n",
        "import gdown\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn import metrics\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# gdown.download('https://drive.google.com/uc?id=1q4U2gVY9tWEPdT6W-pdQpKmo152QqWLE', 'finance_train.csv', True)\n",
        "# gdown.download('https://drive.google.com/uc?id=1nIBqAsItwVEGVayYTgvybz7HeK0asom0', 'finance_test.csv', True)\n",
        "\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_test.csv'\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_train.csv'\n",
        "\n",
        "def get_finance_train():\n",
        "  df_train = pd.read_csv(\"finance_train.csv\")\n",
        "  return df_train\n",
        "def get_finance_test():\n",
        "  df_test = pd.read_csv(\"finance_test.csv\")\n",
        "  return df_test\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "print (\"Train and Test Files Loaded as train.csv and test.csv\")\n",
        "\n",
        "LABEL_MAP = {0 : \"negative\", 1 : \"neutral\", 2 : \"positive\"}\n",
        "NONE = 4 * [None]\n",
        "RND_SEED=2020\n",
        "\n",
        "def plot_confusion_matrix(y_true,y_predicted):\n",
        "  cm = metrics.confusion_matrix(y_true, y_predicted)\n",
        "  print (\"Plotting the Confusion Matrix\")\n",
        "  labels = [\"Negative\",\"Neutral\",\"Positive\"]\n",
        "  df_cm = pd.DataFrame(cm,index =labels,columns = labels)\n",
        "  fig = plt.figure(figsize=(14,12))\n",
        "  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "  plt.yticks([0.5,1.5,2.5], labels,va='center')\n",
        "  plt.title('Confusion Matrix - TestData')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v-QTo5APiwB"
      },
      "source": [
        "\n",
        "df_train = get_finance_train()\n",
        "df_test = get_finance_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO03xnH2S-vB"
      },
      "source": [
        "sentences = df_train['Sentence'].values\n",
        "labels = df_train['Label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FtJFPgrR8jV"
      },
      "source": [
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s12-wmbDCD3"
      },
      "source": [
        "\n",
        "print(tokenizer.vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrw0inSXUjQP"
      },
      "source": [
        "original_sentence = sentences[0]\n",
        "tokenized_sentence = tokenizer.tokenize(original_sentence)\n",
        "print('Original Sentence: ', original_sentence)\n",
        "print('Tokenized Sentence: ', tokenized_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk9a_hV-imMj"
      },
      "source": [
        "original_sentence = sentences[0]\n",
        "tokenized_sentence = tokenizer.tokenize(original_sentence)\n",
        "print('Tokenized Sentence: ', tokenized_sentence)\n",
        "print('Mapped Indices Sentence: ', tokenizer.convert_tokens_to_ids(tokenized_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzuUTbT1pQZf"
      },
      "source": [
        "\n",
        "sentences_with_special_tokens = []\n",
        "for sentence in sentences:\n",
        "  new_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "  sentences_with_special_tokens.append(new_sentence)\n",
        "print(sentences_with_special_tokens[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TE4WkQsrpiy"
      },
      "source": [
        "tokenized_texts = []\n",
        "for sentence in sentences_with_special_tokens:\n",
        "  tokenized_sentence = tokenizer.tokenize(sentence)\n",
        "  tokenized_texts.append(tokenized_sentence)\n",
        "print(tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn4aHXBatKCX"
      },
      "source": [
        "input_ids = []\n",
        "for text in tokenized_texts:\n",
        "  new_list = tokenizer.convert_tokens_to_ids(text)\n",
        "  input_ids.append(new_list)\n",
        "print(input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLcBpb4KtmCu"
      },
      "source": [
        "\n",
        "input_ids = pad_sequences(input_ids,\n",
        "                          maxlen=128,\n",
        "                          dtype=\"long\",\n",
        "                          truncating=\"post\",\n",
        "                          padding=\"post\")\n",
        "print(input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Beyq6gNmw1tS"
      },
      "source": [
        "attention_masks = []\n",
        "for sequence in input_ids:\n",
        "  mask = [float(i > 0) for i in sequence]\n",
        "  attention_masks.append(mask)\n",
        "print (attention_masks[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I85T29scyP7e"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(input_ids,\n",
        "                                                  labels,\n",
        "                                                  test_size=0.15,\n",
        "                                                  random_state=RND_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KeVB2q2zC46"
      },
      "source": [
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks,\n",
        "                                                       input_ids,\n",
        "                                                       test_size=0.15,\n",
        "                                                       random_state=RND_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm8uhq7rU3nY"
      },
      "source": [
        "train_inputs = torch.tensor(np.array(X_train));\n",
        "validation_inputs = torch.tensor(np.array(X_val));\n",
        "train_masks = torch.tensor(np.array(train_masks));\n",
        "validation_masks = torch.tensor(np.array(validation_masks));\n",
        "train_labels = torch.tensor(np.array(y_train));\n",
        "validation_labels = torch.tensor(np.array(y_val));\n",
        "\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels);\n",
        "train_sampler = RandomSampler(train_data); # Samples data randonly for training\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size);\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels);\n",
        "validation_sampler = SequentialSampler(validation_data); # Samples data sequentially\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-iu1Nm0yiTS"
      },
      "source": [
        "#Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT small model, with an uncased vocab.\n",
        "    num_labels = 3,\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ");\n",
        "\n",
        "# Given that this a huge neural network, we need to explicity specify\n",
        "# in pytorch to run this model on the GPU.\n",
        "model.cuda();\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Toc3r89-76M8"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "epochs = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpNhCuKZ33X8"
      },
      "source": [
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "# We'll store training and validation loss,\n",
        "# validation accuracy, and timings.\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "training_stats = []\n",
        "for epoch_i in range(0, epochs):\n",
        "    # Training\n",
        "    print('Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training the model')\n",
        "    # Reset the total loss for  epoch.\n",
        "    total_train_loss = 0\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 20 == 0 and not step == 0:\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n",
        "\n",
        "        # STEP 1 & 2: Unpack this training batch from our dataloader.\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # STEP 3\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass.\n",
        "        model.zero_grad()\n",
        "\n",
        "        # STEP 4\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # It returns the loss (because we provided labels) and\n",
        "        # the \"logits\"--the model outputs prior to activation.\n",
        "        outputs = model(b_input_ids,\n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # STEP 5\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # STEP 6\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    # Validation\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"Evaluating on Validation Set\")\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        #Step 1 and Step 2\n",
        "        # Unpack this validation batch from our dataloader.\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            outputs = model(b_input_ids,\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "\n",
        "    training_loss.append(avg_train_loss)\n",
        "    validation_loss.append(avg_val_loss)\n",
        "    # Record all statistics from this epoch.\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy\n",
        "\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63tvFZsT_ioG"
      },
      "source": [
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.title('Loss over Time')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.plot(training_loss, label='train')\n",
        "plt.plot(validation_loss, label='validation')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNy6r6m6BLjZ"
      },
      "source": [
        "\n",
        "test_sentences = df_test.Sentence.values\n",
        "test_labels = df_test.Label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ncZome9-tPE"
      },
      "source": [
        "test_input_ids, test_attention_masks = [], []\n",
        "\n",
        "# Add Special Tokens\n",
        "test_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in test_sentences]\n",
        "\n",
        "# Tokenize sentences\n",
        "tokenized_test_sentences = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
        "\n",
        "# Encode Tokens to Word IDs\n",
        "test_input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_test_sentences]\n",
        "\n",
        "# Pad the inputs\n",
        "test_input_ids = pad_sequences(test_input_ids,\n",
        "                               maxlen=128,\n",
        "                               dtype=\"long\",\n",
        "                               truncating=\"post\",\n",
        "                               padding=\"post\")\n",
        "\n",
        "# Create Attention Masks\n",
        "for sequence in test_input_ids:\n",
        "  mask = [float(i>0) for i in sequence]\n",
        "  test_attention_masks.append(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CiYwcQIANi8"
      },
      "source": [
        "batch_size = 32\n",
        "test_input_ids = torch.tensor(test_input_ids)\n",
        "test_attention_masks = torch.tensor(test_attention_masks)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4rX-vG7AaXl"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "# Final tracking variables\n",
        "y_logits, y_true, y_preds = [], [], []\n",
        "\n",
        "# Gather logit predictions\n",
        "for chunk in predictions:\n",
        "  for logits in chunk:\n",
        "    y_logits.append(logits)\n",
        "\n",
        "# Gather true labels\n",
        "for chunk in true_labels:\n",
        "  for label in chunk:\n",
        "    y_true.append(label)\n",
        "\n",
        "# Gather real predictions\n",
        "for logits in y_logits:\n",
        "  y_preds.append(np.argmax(logits))\n",
        "\n",
        "print ('Test Accuracy: {:.2%}'.format(metrics.accuracy_score(y_preds,y_true)))\n",
        "plot_confusion_matrix(y_true,y_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKtr5xWpzflg"
      },
      "source": [
        "<img src=\"https://www.sr-sv.com/wp-content/uploads/2019/06/NLP_0000.jpg\" width=\"800\">"
      ]
    }
  ]
}